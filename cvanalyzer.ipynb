{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15af8ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (751 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, scrolledtext, ttk, messagebox\n",
    "import PyPDF2\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Global variables for models\n",
    "nlp = None\n",
    "grammar_model = None\n",
    "similarity_model = None\n",
    "models_loaded = False\n",
    "\n",
    "# Define resume sections with better keywords\n",
    "SECTION_TEMPLATES = {\n",
    "    \"Contact Information\": [\"phone\", \"email\", \"address\", \"linkedin\", \"contact\", \"mobile\", \"tel\", \"@\"],\n",
    "    \"Professional Summary\": [\"summary\", \"profile\", \"objective\", \"about\", \"overview\", \"career goal\"],\n",
    "    \"Education\": [\"education\", \"degree\", \"university\", \"college\", \"school\", \"bachelor\", \"master\", \"phd\", \"graduated\", \"gpa\", \"academic\"],\n",
    "    \"Work Experience\": [\"experience\", \"employment\", \"work\", \"job\", \"position\", \"role\", \"company\", \"worked\", \"responsibilities\", \"achievements\"],\n",
    "    \"Skills\": [\"skills\", \"technical\", \"proficient\", \"expertise\", \"programming\", \"languages\", \"tools\", \"technologies\", \"competencies\"],\n",
    "    \"Projects\": [\"projects\", \"developed\", \"built\", \"created\", \"implemented\", \"designed\", \"research\", \"portfolio\"],\n",
    "    \"Certifications\": [\"certification\", \"certified\", \"license\", \"credential\", \"certificate\", \"award\", \"recognition\"],\n",
    "    \"Languages\": [\"languages\", \"fluent\", \"native\", \"bilingual\", \"multilingual\", \"spanish\", \"french\", \"german\"]\n",
    "}\n",
    "\n",
    "# Improved weak phrases detection\n",
    "WEAK_PHRASES = [\n",
    "    'some', 'various', 'etc.', 'responsible for', 'duties include', 'tasks include',\n",
    "    'helped with', 'assisted with', 'worked on', 'dealt with', 'handled',\n",
    "    'many', 'several', 'numerous', 'stuff', 'things', 'good at', 'familiar with'\n",
    "]\n",
    "\n",
    "# Strong action words for suggestions\n",
    "STRONG_ACTION_WORDS = [\n",
    "    'achieved', 'implemented', 'developed', 'created', 'designed', 'managed',\n",
    "    'led', 'improved', 'increased', 'reduced', 'optimized', 'streamlined',\n",
    "    'collaborated', 'coordinated', 'executed', 'delivered', 'established',\n",
    "    'initiated', 'launched', 'maintained', 'mentored', 'negotiated'\n",
    "]\n",
    "\n",
    "# ---------------- Model Loading Functions ----------------\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"Load all NLP models in a separate thread\"\"\"\n",
    "    global nlp, grammar_model, similarity_model, models_loaded\n",
    "    \n",
    "    try:\n",
    "        status_label.config(text=\"üîÑ Loading spaCy model...\", fg=\"#fd7e14\")\n",
    "        root.update()\n",
    "        \n",
    "        # Load spaCy model\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        status_label.config(text=\"üîÑ Loading grammar correction model...\", fg=\"#fd7e14\")\n",
    "        root.update()\n",
    "        \n",
    "        # Load grammar correction model\n",
    "        grammar_model = pipeline(\"text2text-generation\", model=\"prithivida/grammar_error_correcter_v1\")\n",
    "        \n",
    "        status_label.config(text=\"üîÑ Loading similarity model...\", fg=\"#fd7e14\")\n",
    "        root.update()\n",
    "        \n",
    "        # Load sentence transformer\n",
    "        similarity_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        models_loaded = True\n",
    "        status_label.config(text=\"‚úÖ All models loaded successfully! Ready to analyze.\", fg=\"#198754\")\n",
    "        upload_button.config(state=\"normal\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Model Loading Error\", f\"Failed to load models: {str(e)}\")\n",
    "        status_label.config(text=\"‚ùå Error loading models\", fg=\"#dc3545\")\n",
    "\n",
    "def start_model_loading():\n",
    "    \"\"\"Start model loading in a separate thread\"\"\"\n",
    "    thread = threading.Thread(target=load_models)\n",
    "    thread.daemon = True\n",
    "    thread.start()\n",
    "\n",
    "# ---------------- Core Functions ----------------\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"Extract text from PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                content = page.extract_text()\n",
    "                if content:\n",
    "                    text += content + \"\\n\"\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error Reading PDF\", f\"Could not read PDF file: {str(e)}\")\n",
    "    return text.strip()\n",
    "\n",
    "def correct_grammar(text):\n",
    "    \"\"\"Improve grammar correction with better chunking\"\"\"\n",
    "    if not grammar_model:\n",
    "        return text\n",
    "    \n",
    "    # Split into sentences\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    corrected_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(sentence.strip()) > 0:\n",
    "            try:\n",
    "                # Clean the sentence\n",
    "                cleaned = sentence.strip()\n",
    "                if len(cleaned) > 5:  # Only process meaningful sentences\n",
    "                    # Use the grammar model\n",
    "                    result = grammar_model(cleaned, max_length=len(cleaned) + 50, do_sample=False)\n",
    "                    if result and len(result) > 0:\n",
    "                        corrected = result[0]['generated_text'].strip()\n",
    "                        corrected_sentences.append(corrected)\n",
    "                    else:\n",
    "                        corrected_sentences.append(cleaned)\n",
    "                else:\n",
    "                    corrected_sentences.append(cleaned)\n",
    "            except Exception as e:\n",
    "                print(f\"Grammar correction error: {e}\")\n",
    "                corrected_sentences.append(sentence.strip())\n",
    "    \n",
    "    return \" \".join(corrected_sentences)\n",
    "\n",
    "def detect_passive_sentences(text):\n",
    "    \"\"\"Detect passive voice sentences using spaCy\"\"\"\n",
    "    if not nlp:\n",
    "        return []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    passive_sentences = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        # Look for passive voice indicators\n",
    "        has_passive = False\n",
    "        for token in sent:\n",
    "            # Check for auxiliary verbs with past participles\n",
    "            if token.dep_ == \"auxpass\" or (token.lemma_ in [\"be\", \"get\"] and any(child.tag_ == \"VBN\" for child in token.children)):\n",
    "                has_passive = True\n",
    "                break\n",
    "        \n",
    "        if has_passive:\n",
    "            passive_sentences.append(sent.text.strip())\n",
    "    \n",
    "    return passive_sentences\n",
    "\n",
    "def detect_weak_phrases(text):\n",
    "    \"\"\"Detect weak/vague phrases in the text\"\"\"\n",
    "    found_phrases = []\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for phrase in WEAK_PHRASES:\n",
    "        if phrase.lower() in text_lower:\n",
    "            found_phrases.append(phrase)\n",
    "    \n",
    "    return found_phrases\n",
    "\n",
    "def detect_sections_improved(text):\n",
    "    \"\"\"Improved section detection using multiple methods\"\"\"\n",
    "    detected_sections = {}\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Method 1: Direct keyword matching\n",
    "    for section, keywords in SECTION_TEMPLATES.items():\n",
    "        score = 0\n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in text_lower:\n",
    "                score += 1\n",
    "        \n",
    "        # If we found enough keywords, consider section present\n",
    "        if score >= 1:  # Lowered threshold for better detection\n",
    "            detected_sections[section] = score\n",
    "    \n",
    "    # Method 2: Header-based detection\n",
    "    lines = text.split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        line_clean = line.strip().lower()\n",
    "        if len(line_clean) > 0:\n",
    "            for section, keywords in SECTION_TEMPLATES.items():\n",
    "                for keyword in keywords:\n",
    "                    if keyword in line_clean and len(line_clean) < 50:  # Likely a header\n",
    "                        detected_sections[section] = detected_sections.get(section, 0) + 2\n",
    "    \n",
    "    # Method 3: Semantic similarity (if model is available)\n",
    "    if similarity_model:\n",
    "        try:\n",
    "            paragraphs = [p.strip() for p in re.split(r'\\n\\s*\\n', text) if p.strip()]\n",
    "            for section, keywords in SECTION_TEMPLATES.items():\n",
    "                if section not in detected_sections:\n",
    "                    section_desc = \" \".join(keywords)\n",
    "                    section_embedding = similarity_model.encode(section_desc)\n",
    "                    \n",
    "                    max_similarity = 0\n",
    "                    for para in paragraphs:\n",
    "                        if len(para) > 20:  # Skip very short paragraphs\n",
    "                            para_embedding = similarity_model.encode(para)\n",
    "                            similarity = similarity_model.similarity([section_embedding], [para_embedding])[0][0]\n",
    "                            max_similarity = max(max_similarity, float(similarity))\n",
    "                    \n",
    "                    if max_similarity > 0.3:  # Lowered threshold\n",
    "                        detected_sections[section] = max_similarity\n",
    "        except Exception as e:\n",
    "            print(f\"Similarity detection error: {e}\")\n",
    "    \n",
    "    return detected_sections\n",
    "\n",
    "def get_improvement_suggestions(text, detected_sections, passive_sentences, weak_phrases):\n",
    "    \"\"\"Generate improvement suggestions\"\"\"\n",
    "    suggestions = []\n",
    "    \n",
    "    # Word count suggestions\n",
    "    word_count = len(text.split())\n",
    "    if word_count < 200:\n",
    "        suggestions.append(\"üìù Add more detail to your experiences and achievements (current: {} words)\".format(word_count))\n",
    "    elif word_count > 800:\n",
    "        suggestions.append(\"‚úÇÔ∏è Consider condensing content to make it more concise (current: {} words)\".format(word_count))\n",
    "    \n",
    "    # Section suggestions\n",
    "    missing_sections = []\n",
    "    for section in SECTION_TEMPLATES:\n",
    "        if section not in detected_sections:\n",
    "            missing_sections.append(section)\n",
    "    \n",
    "    if missing_sections:\n",
    "        suggestions.append(\"üìÇ Add missing sections: {}\".format(', '.join(missing_sections)))\n",
    "    \n",
    "    # Passive voice suggestions\n",
    "    if passive_sentences:\n",
    "        suggestions.append(\"üé≠ Replace {} passive voice sentences with active voice for stronger impact\".format(len(passive_sentences)))\n",
    "    \n",
    "    # Weak phrases suggestions\n",
    "    if weak_phrases:\n",
    "        suggestions.append(\"üí™ Replace weak phrases like '{}' with stronger action words\".format(', '.join(weak_phrases[:3])))\n",
    "    \n",
    "    # Action words suggestion\n",
    "    action_word_count = sum(1 for word in STRONG_ACTION_WORDS if word in text.lower())\n",
    "    if action_word_count < 5:\n",
    "        suggestions.append(\"üöÄ Use more strong action words (achieved, implemented, developed, managed, etc.)\")\n",
    "    \n",
    "    # Quantification suggestion\n",
    "    numbers = re.findall(r'\\d+', text)\n",
    "    if len(numbers) < 3:\n",
    "        suggestions.append(\"üìä Add more quantifiable achievements (numbers, percentages, metrics)\")\n",
    "    \n",
    "    # Contact information check\n",
    "    if \"Contact Information\" not in detected_sections:\n",
    "        suggestions.append(\"üìû Ensure your contact information (phone, email, LinkedIn) is clearly visible\")\n",
    "    \n",
    "    # Skills section enhancement\n",
    "    if \"Skills\" in detected_sections:\n",
    "        suggestions.append(\"üõ†Ô∏è Group technical skills by category (Programming, Tools, Languages, etc.)\")\n",
    "    \n",
    "    return suggestions\n",
    "\n",
    "def calculate_resume_score(text, detected_sections, passive_sentences, weak_phrases):\n",
    "    \"\"\"Calculate comprehensive resume score\"\"\"\n",
    "    score = 100.0\n",
    "    \n",
    "    # Word count scoring (20 points)\n",
    "    word_count = len(text.split())\n",
    "    if word_count < 150:\n",
    "        score -= 15\n",
    "    elif word_count < 250:\n",
    "        score -= 8\n",
    "    elif word_count > 800:\n",
    "        score -= 5\n",
    "    \n",
    "    # Section completeness scoring (30 points)\n",
    "    section_score = (len(detected_sections) / len(SECTION_TEMPLATES)) * 30\n",
    "    score = score - 30 + section_score\n",
    "    \n",
    "    # Passive voice penalty (15 points)\n",
    "    passive_penalty = min(len(passive_sentences) * 2, 15)\n",
    "    score -= passive_penalty\n",
    "    \n",
    "    # Weak phrases penalty (15 points)\n",
    "    weak_penalty = min(len(weak_phrases) * 3, 15)\n",
    "    score -= weak_penalty\n",
    "    \n",
    "    # Action words bonus (10 points)\n",
    "    action_words = sum(1 for word in STRONG_ACTION_WORDS if word in text.lower())\n",
    "    action_bonus = min(action_words * 1, 10)\n",
    "    score += action_bonus - 10  # Start with -10, add back based on action words\n",
    "    \n",
    "    # Quantification bonus (10 points)\n",
    "    numbers = re.findall(r'\\d+', text)\n",
    "    quant_bonus = min(len(numbers) * 2, 10)\n",
    "    score += quant_bonus - 10  # Start with -10, add back based on numbers\n",
    "    \n",
    "    return max(0, min(100, round(score, 1)))\n",
    "\n",
    "# ---------------- GUI Functions ----------------\n",
    "\n",
    "def upload_file():\n",
    "    \"\"\"Handle file upload and analysis\"\"\"\n",
    "    if not models_loaded:\n",
    "        messagebox.showwarning(\"Models Not Loaded\", \"Please wait for models to load before uploading.\")\n",
    "        return\n",
    "    \n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select Resume PDF\",\n",
    "        filetypes=[(\"PDF Files\", \"*.pdf\"), (\"All Files\", \"*.*\")]\n",
    "    )\n",
    "    \n",
    "    if not file_path:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Update status\n",
    "        status_label.config(text=\"üìÑ Analyzing resume... Please wait.\", fg=\"#fd7e14\")\n",
    "        root.update()\n",
    "        \n",
    "        # Extract text\n",
    "        raw_text = extract_text_from_pdf(file_path)\n",
    "        if not raw_text:\n",
    "            messagebox.showerror(\"Error\", \"Could not extract text from PDF. Please ensure it's a valid PDF file.\")\n",
    "            return\n",
    "        \n",
    "        # Display original text\n",
    "        text_area.delete(1.0, tk.END)\n",
    "        text_area.insert(tk.END, raw_text)\n",
    "        \n",
    "        # Grammar correction\n",
    "        status_label.config(text=\"‚úçÔ∏è Correcting grammar...\", fg=\"#fd7e14\")\n",
    "        root.update()\n",
    "        corrected_text = correct_grammar(raw_text)\n",
    "        \n",
    "        correction_output.config(state=\"normal\")\n",
    "        correction_output.delete(1.0, tk.END)\n",
    "        correction_output.insert(tk.END, corrected_text)\n",
    "        correction_output.config(state=\"disabled\")\n",
    "        \n",
    "        # Detect passive sentences\n",
    "        passive_sentences = detect_passive_sentences(raw_text)\n",
    "        passive_output.config(state=\"normal\")\n",
    "        passive_output.delete(1.0, tk.END)\n",
    "        if passive_sentences:\n",
    "            passive_output.insert(tk.END, \"Found {} passive voice sentences:\\n\\n\".format(len(passive_sentences)))\n",
    "            for i, sentence in enumerate(passive_sentences, 1):\n",
    "                passive_output.insert(tk.END, f\"{i}. {sentence}\\n\\n\")\n",
    "        else:\n",
    "            passive_output.insert(tk.END, \"‚úÖ Excellent! No passive voice detected in your resume.\")\n",
    "        passive_output.config(state=\"disabled\")\n",
    "        \n",
    "        # Detect weak phrases\n",
    "        weak_phrases = detect_weak_phrases(raw_text)\n",
    "        weak_output.config(state=\"normal\")\n",
    "        weak_output.delete(1.0, tk.END)\n",
    "        if weak_phrases:\n",
    "            weak_output.insert(tk.END, \"Found {} weak phrases:\\n\\n\".format(len(weak_phrases)))\n",
    "            for i, phrase in enumerate(weak_phrases, 1):\n",
    "                weak_output.insert(tk.END, f\"{i}. '{phrase}'\\n\")\n",
    "        else:\n",
    "            weak_output.insert(tk.END, \"‚úÖ Great! No weak phrases detected.\")\n",
    "        weak_output.config(state=\"disabled\")\n",
    "        \n",
    "        # Detect sections\n",
    "        detected_sections = detect_sections_improved(raw_text)\n",
    "        section_output.config(state=\"normal\")\n",
    "        section_output.delete(1.0, tk.END)\n",
    "        \n",
    "        present_sections = list(detected_sections.keys())\n",
    "        missing_sections = [sec for sec in SECTION_TEMPLATES if sec not in detected_sections]\n",
    "        \n",
    "        if present_sections:\n",
    "            section_output.insert(tk.END, \"‚úÖ DETECTED SECTIONS ({}/{}):\".format(len(present_sections), len(SECTION_TEMPLATES)))\n",
    "            section_output.insert(tk.END, \"\\n\" + \"=\"*50 + \"\\n\")\n",
    "            for section in present_sections:\n",
    "                section_output.insert(tk.END, f\"‚Ä¢ {section}\\n\")\n",
    "        \n",
    "        if missing_sections:\n",
    "            section_output.insert(tk.END, \"\\n‚ùå MISSING SECTIONS ({}):\\n\".format(len(missing_sections)))\n",
    "            section_output.insert(tk.END, \"=\"*50 + \"\\n\")\n",
    "            for section in missing_sections:\n",
    "                section_output.insert(tk.END, f\"‚Ä¢ {section}\\n\")\n",
    "        \n",
    "        if not missing_sections:\n",
    "            section_output.insert(tk.END, \"\\nüéâ All sections detected! Your resume is well-structured.\")\n",
    "        \n",
    "        section_output.config(state=\"disabled\")\n",
    "        \n",
    "        # Calculate score\n",
    "        score = calculate_resume_score(raw_text, detected_sections, passive_sentences, weak_phrases)\n",
    "        \n",
    "        # Update score display\n",
    "        if score >= 80:\n",
    "            score_color = \"#198754\"  # Green\n",
    "            score_emoji = \"üåü\"\n",
    "            score_rating = \"Excellent\"\n",
    "        elif score >= 60:\n",
    "            score_color = \"#ffc107\"  # Yellow\n",
    "            score_emoji = \"üìà\"\n",
    "            score_rating = \"Good\"\n",
    "        else:\n",
    "            score_color = \"#dc3545\"  # Red\n",
    "            score_emoji = \"üìâ\"\n",
    "            score_rating = \"Needs Improvement\"\n",
    "        \n",
    "        score_label.config(text=f\"{score_emoji} {score_rating}: {score}/100\", fg=score_color)\n",
    "        \n",
    "        # Generate and display suggestions\n",
    "        suggestions = get_improvement_suggestions(raw_text, detected_sections, passive_sentences, weak_phrases)\n",
    "        suggestions_output.config(state=\"normal\")\n",
    "        suggestions_output.delete(1.0, tk.END)\n",
    "        \n",
    "        if suggestions:\n",
    "            suggestions_output.insert(tk.END, \"üí° IMPROVEMENT SUGGESTIONS:\\n\")\n",
    "            suggestions_output.insert(tk.END, \"=\"*60 + \"\\n\\n\")\n",
    "            for i, suggestion in enumerate(suggestions, 1):\n",
    "                suggestions_output.insert(tk.END, f\"{i}. {suggestion}\\n\\n\")\n",
    "        else:\n",
    "            suggestions_output.insert(tk.END, \"üéâ CONGRATULATIONS!\\n\")\n",
    "            suggestions_output.insert(tk.END, \"=\"*60 + \"\\n\\n\")\n",
    "            suggestions_output.insert(tk.END, \"Your resume looks excellent! No major improvements needed.\")\n",
    "        \n",
    "        suggestions_output.config(state=\"disabled\")\n",
    "        \n",
    "        status_label.config(text=\"‚úÖ Analysis completed successfully!\", fg=\"#198754\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Analysis Error\", f\"An error occurred during analysis: {str(e)}\")\n",
    "        status_label.config(text=\"‚ùå Analysis failed\", fg=\"#dc3545\")\n",
    "\n",
    "def clear_all():\n",
    "    \"\"\"Clear all text areas and reset the interface\"\"\"\n",
    "    text_area.delete(1.0, tk.END)\n",
    "    correction_output.config(state=\"normal\")\n",
    "    correction_output.delete(1.0, tk.END)\n",
    "    correction_output.config(state=\"disabled\")\n",
    "    \n",
    "    passive_output.config(state=\"normal\")\n",
    "    passive_output.delete(1.0, tk.END)\n",
    "    passive_output.config(state=\"disabled\")\n",
    "    \n",
    "    weak_output.config(state=\"normal\")\n",
    "    weak_output.delete(1.0, tk.END)\n",
    "    weak_output.config(state=\"disabled\")\n",
    "    \n",
    "    section_output.config(state=\"normal\")\n",
    "    section_output.delete(1.0, tk.END)\n",
    "    section_output.config(state=\"disabled\")\n",
    "    \n",
    "    suggestions_output.config(state=\"normal\")\n",
    "    suggestions_output.delete(1.0, tk.END)\n",
    "    suggestions_output.config(state=\"disabled\")\n",
    "    \n",
    "    score_label.config(text=\"Resume Score: Not analyzed\", fg=\"#6c757d\")\n",
    "    status_label.config(text=\"‚úÖ Ready to analyze new resume\", fg=\"#198754\")\n",
    "\n",
    "# ---------------- UI Layout ----------------\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"üéì AI Resume Analyzer - Professional Edition\")\n",
    "root.geometry(\"1400x900\")\n",
    "root.config(bg=\"#f8f9fa\")\n",
    "root.state('zoomed')  # Maximize window on Windows\n",
    "\n",
    "# Configure styles\n",
    "style = ttk.Style()\n",
    "style.theme_use(\"clam\")\n",
    "\n",
    "# Header\n",
    "header_frame = tk.Frame(root, bg=\"#0d6efd\", height=100)\n",
    "header_frame.pack(fill=\"x\")\n",
    "header_frame.pack_propagate(False)\n",
    "\n",
    "title_label = tk.Label(header_frame, text=\"üéì AI Resume Analyzer\", \n",
    "                      font=(\"Arial\", 28, \"bold\"), fg=\"white\", bg=\"#0d6efd\")\n",
    "title_label.pack(pady=(15, 5))\n",
    "\n",
    "subtitle_label = tk.Label(header_frame, text=\"Professional NLP-powered resume analysis with grammar correction and improvement suggestions\", \n",
    "                         font=(\"Arial\", 12), fg=\"white\", bg=\"#0d6efd\")\n",
    "subtitle_label.pack()\n",
    "\n",
    "# Control panel\n",
    "control_frame = tk.Frame(root, bg=\"#f8f9fa\", height=60)\n",
    "control_frame.pack(fill=\"x\", padx=20, pady=10)\n",
    "control_frame.pack_propagate(False)\n",
    "\n",
    "upload_button = tk.Button(control_frame, text=\"üìÅ Upload Resume (PDF)\", \n",
    "                         command=upload_file, state=\"disabled\",\n",
    "                         font=(\"Arial\", 12, \"bold\"), bg=\"#0d6efd\", fg=\"white\",\n",
    "                         padx=20, pady=8, relief=\"flat\")\n",
    "upload_button.pack(side=\"left\", padx=(0, 20))\n",
    "\n",
    "clear_button = tk.Button(control_frame, text=\"üóëÔ∏è Clear All\", \n",
    "                        command=clear_all,\n",
    "                        font=(\"Arial\", 12), bg=\"#dc3545\", fg=\"white\",\n",
    "                        padx=20, pady=8, relief=\"flat\")\n",
    "clear_button.pack(side=\"left\", padx=(0, 20))\n",
    "\n",
    "status_label = tk.Label(control_frame, text=\"üîÑ Initializing models...\", \n",
    "                       font=(\"Arial\", 12, \"bold\"), fg=\"#fd7e14\", bg=\"#f8f9fa\")\n",
    "status_label.pack(side=\"left\")\n",
    "\n",
    "# Main content area with notebook (tabs)\n",
    "notebook = ttk.Notebook(root)\n",
    "notebook.pack(fill=\"both\", expand=True, padx=20, pady=10)\n",
    "\n",
    "# Tab 1: Text Content\n",
    "text_frame = tk.Frame(notebook, bg=\"#f8f9fa\")\n",
    "notebook.add(text_frame, text=\"üìù Resume Content\")\n",
    "\n",
    "# Create paned window for text content\n",
    "text_paned = tk.PanedWindow(text_frame, orient=\"horizontal\", sashrelief=\"raised\")\n",
    "text_paned.pack(fill=\"both\", expand=True, padx=10, pady=10)\n",
    "\n",
    "# Original text panel\n",
    "original_frame = tk.Frame(text_paned, bg=\"white\", relief=\"raised\", bd=2)\n",
    "text_paned.add(original_frame, width=600)\n",
    "\n",
    "tk.Label(original_frame, text=\"üìù Original Resume Text\", \n",
    "         font=(\"Arial\", 14, \"bold\"), bg=\"white\", fg=\"#0d6efd\").pack(pady=(10, 5))\n",
    "\n",
    "text_area = scrolledtext.ScrolledText(original_frame, wrap=tk.WORD, \n",
    "                                     font=(\"Consolas\", 11), bg=\"#f8f9fa\", \n",
    "                                     relief=\"sunken\", bd=1)\n",
    "text_area.pack(padx=10, pady=(0, 10), fill=\"both\", expand=True)\n",
    "\n",
    "# Grammar corrected text panel\n",
    "corrected_frame = tk.Frame(text_paned, bg=\"white\", relief=\"raised\", bd=2)\n",
    "text_paned.add(corrected_frame, width=600)\n",
    "\n",
    "tk.Label(corrected_frame, text=\"‚úÖ Grammar Corrected Version\", \n",
    "         font=(\"Arial\", 14, \"bold\"), bg=\"white\", fg=\"#198754\").pack(pady=(10, 5))\n",
    "\n",
    "correction_output = scrolledtext.ScrolledText(corrected_frame, wrap=tk.WORD, \n",
    "                                            state=\"disabled\", font=(\"Consolas\", 11), \n",
    "                                            bg=\"#f0fff0\", relief=\"sunken\", bd=1)\n",
    "correction_output.pack(padx=10, pady=(0, 10), fill=\"both\", expand=True)\n",
    "\n",
    "# Tab 2: Analysis Results\n",
    "analysis_frame = tk.Frame(notebook, bg=\"#f8f9fa\")\n",
    "notebook.add(analysis_frame, text=\"üìä Analysis Results\")\n",
    "\n",
    "# Score display at top\n",
    "score_frame = tk.Frame(analysis_frame, bg=\"#f8f9fa\", height=80)\n",
    "score_frame.pack(fill=\"x\", pady=(10, 0))\n",
    "score_frame.pack_propagate(False)\n",
    "\n",
    "score_label = tk.Label(score_frame, text=\"Resume Score: Not analyzed\", \n",
    "                      font=(\"Arial\", 24, \"bold\"), bg=\"#f8f9fa\", fg=\"#6c757d\")\n",
    "score_label.pack(pady=20)\n",
    "\n",
    "# Create grid for analysis results\n",
    "analysis_grid = tk.Frame(analysis_frame, bg=\"#f8f9fa\")\n",
    "analysis_grid.pack(fill=\"both\", expand=True, padx=10, pady=10)\n",
    "\n",
    "# Configure grid weights\n",
    "analysis_grid.grid_rowconfigure(0, weight=1)\n",
    "analysis_grid.grid_columnconfigure(0, weight=1)\n",
    "analysis_grid.grid_columnconfigure(1, weight=1)\n",
    "\n",
    "# Left column\n",
    "left_analysis = tk.Frame(analysis_grid, bg=\"white\", relief=\"raised\", bd=2)\n",
    "left_analysis.grid(row=0, column=0, sticky=\"nsew\", padx=(0, 5))\n",
    "\n",
    "# Passive voice\n",
    "tk.Label(left_analysis, text=\"üé≠ Passive Voice Detection\", \n",
    "         font=(\"Arial\", 14, \"bold\"), bg=\"white\", fg=\"#fd7e14\").pack(pady=(15, 5))\n",
    "\n",
    "passive_output = scrolledtext.ScrolledText(left_analysis, height=10, state=\"disabled\", \n",
    "                                          wrap=tk.WORD, font=(\"Consolas\", 10), \n",
    "                                          bg=\"#fff3cd\", relief=\"sunken\", bd=1)\n",
    "passive_output.pack(padx=15, pady=(0, 15), fill=\"both\", expand=True)\n",
    "\n",
    "# Weak phrases\n",
    "tk.Label(left_analysis, text=\"üí§ Weak/Vague Phrases\", \n",
    "         font=(\"Arial\", 14, \"bold\"), bg=\"white\", fg=\"#dc3545\").pack(pady=(0, 5))\n",
    "\n",
    "weak_output = scrolledtext.ScrolledText(left_analysis, height=8, state=\"disabled\", \n",
    "                                       wrap=tk.WORD, font=(\"Consolas\", 10), \n",
    "                                       bg=\"#f8d7da\", relief=\"sunken\", bd=1)\n",
    "weak_output.pack(padx=15, pady=(0, 15), fill=\"both\", expand=True)\n",
    "\n",
    "# Right column\n",
    "right_analysis = tk.Frame(analysis_grid, bg=\"white\", relief=\"raised\", bd=2)\n",
    "right_analysis.grid(row=0, column=1, sticky=\"nsew\", padx=(5, 0))\n",
    "\n",
    "# Section analysis\n",
    "tk.Label(right_analysis, text=\"üìÇ Section Analysis\", \n",
    "         font=(\"Arial\", 14, \"bold\"), bg=\"white\", fg=\"#0d6efd\").pack(pady=(15, 5))\n",
    "\n",
    "section_output = scrolledtext.ScrolledText(right_analysis, height=10, state=\"disabled\", \n",
    "                                          wrap=tk.WORD, font=(\"Consolas\", 10), \n",
    "                                          bg=\"#e7f3ff\", relief=\"sunken\", bd=1)\n",
    "section_output.pack(padx=15, pady=(0, 15), fill=\"both\", expand=True)\n",
    "\n",
    "# Improvement suggestions\n",
    "tk.Label(right_analysis, text=\"üí° Improvement Suggestions\", \n",
    "         font=(\"Arial\", 14, \"bold\"), bg=\"white\", fg=\"#198754\").pack(pady=(0, 5))\n",
    "\n",
    "suggestions_output = scrolledtext.ScrolledText(right_analysis, height=8, state=\"disabled\", \n",
    "                                              wrap=tk.WORD, font=(\"Arial\", 11), \n",
    "                                              bg=\"#d4edda\", relief=\"sunken\", bd=1)\n",
    "suggestions_output.pack(padx=15, pady=(0, 15), fill=\"both\", expand=True)\n",
    "\n",
    "# Footer\n",
    "footer_frame = tk.Frame(root, bg=\"#e9ecef\", height=40)\n",
    "footer_frame.pack(fill=\"x\", side=\"bottom\")\n",
    "footer_frame.pack_propagate(False)\n",
    "\n",
    "tk.Label(footer_frame, text=\"¬© 2025 AI Resume Analyzer | Final Year Project | Built with ‚ù§Ô∏è using Transformers, spaCy & Tkinter\", \n",
    "         font=(\"Arial\", 10), bg=\"#e9ecef\", fg=\"#6c757d\").pack(pady=12)\n",
    "\n",
    "# Start model loading\n",
    "start_model_loading()\n",
    "\n",
    "# Start the GUI\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed24903e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
